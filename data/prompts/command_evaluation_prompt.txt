You are evaluating bash commands in a conversational assistant context.

Expected Command (reference):
{expected}

Generated Command:
{generated}

Task: Evaluate the generated command on two dimensions:

1. SEMANTIC EQUIVALENCE: Determine if the generated command is semantically equivalent to the expected command.
   Two commands are semantically equivalent if they achieve the same goal, even if the exact syntax differs.

2. CORRECTNESS: Evaluate if the generated command is a CORRECT response to the conversation,
   considering the user's request and the current state. The command doesn't need to match
   the expected command exactly, but it should be a valid and appropriate response.

Respond in JSON format:
{{
    "semantic_equivalence": {{
        "score": <float between 0 and 1>,
        "reasoning": "<brief explanation>"
    }},
    "correctness": {{
        "score": <float between 0 and 1>,
        "reasoning": "<brief explanation>",
        "is_valid_syntax": <true/false>,
        "addresses_user_request": <true/false>
    }}
}}

Score Guidelines for Semantic Equivalence:
- 1.0: Commands are functionally identical or achieve the exact same result
- 0.7-0.9: Commands achieve the same goal with minor differences (e.g., different flags, equivalent syntax)
- 0.4-0.6: Commands are related but have meaningful differences
- 0.0-0.3: Commands are significantly different or incorrect

Score Guidelines for Correctness:
- 1.0: Command is correct, safe, and fully addresses the user's request
- 0.7-0.9: Command is mostly correct with minor issues
- 0.4-0.6: Command is partially correct but has significant issues
- 0.0-0.3: Command is incorrect, unsafe, or doesn't address the request

